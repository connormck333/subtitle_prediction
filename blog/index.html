<!DOCTYPE html>
<!-- This site was created in Webflow. https://www.webflow.com --><!-- Last Published: Sun Apr 23 2023 15:46:44 GMT+0000 (Coordinated Universal Time) -->
<html data-wf-domain="blog-post-6fd454.webflow.io" data-wf-page="63ff4af898570038e266e160"
    data-wf-site="63ff4af7985700a35d66e15f">

<head>
    <meta charset="utf-8" />
    <title>blog-post</title>
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <meta content="Webflow" name="generator" />
    <link href="./styles.css"
        rel="stylesheet" type="text/css" />
    <link href="https://fonts.googleapis.com" rel="preconnect" />
    <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin="anonymous" />
    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
    <script type="text/javascript">WebFont.load({ google: { families: ["Inconsolata:400,700"] } });</script>
    <!--[if lt IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" type="text/javascript"></script><![endif]-->
    <script
        type="text/javascript">!function (o, c) { var n = c.documentElement, t = " w-mod-"; n.className += t + "js", ("ontouchstart" in o || o.DocumentTouch && c instanceof DocumentTouch) && (n.className += t + "touch") }(window, document);</script>
    <link href="https://uploads-ssl.webflow.com/img/favicon.ico" rel="shortcut icon" type="image/x-icon" />
    <link href="https://uploads-ssl.webflow.com/img/webclip.png" rel="apple-touch-icon" />
</head>

<body>
    <section class="hero-stack wf-section">
        <div class="container">
            <div class="hero-wrapper-two">
                <h1>Identifying speakers in movie clips using Python</h1>
                <p class="margin-bottom-24px">Using Python to create Artificial Intelligence to detect speakers in movie
                    clips using face recognition.</p><img
                    src="https://uploads-ssl.webflow.com/63ff4af7985700a35d66e15f/6400d7b2267482a427c637d3_face-600x900.png"
                    loading="lazy"
                    sizes="(max-width: 479px) 94vw, (max-width: 767px) 95vw, (max-width: 991px) 92vw, 750px"
                    srcset="https://uploads-ssl.webflow.com/63ff4af7985700a35d66e15f/6400d7b2267482a427c637d3_face-600x900-p-500.png 500w, https://uploads-ssl.webflow.com/63ff4af7985700a35d66e15f/6400d7b2267482a427c637d3_face-600x900-p-800.png 800w, https://uploads-ssl.webflow.com/63ff4af7985700a35d66e15f/6400d7b2267482a427c637d3_face-600x900.png 900w"
                    alt="" class="hero-image shadow-two" />
            </div>
        </div>
    </section>
    <div class="w-container">
        <h2>The problem</h2>
        <p class="paragraph-3">Recently, I had been watching a YouTube clip in spanish to try and see how much I could
            remember from my spanish A-Level. I turned on the auto-generated subtitles and found it difficult to figure
            out who was saying each line. I realised that there needs to be a better system in place.<br /><br />What if
            I could create a program that can identify the speakers in a clip using face-recognition AI? The speaker can
            then be added to the auto-generated subtitles making it much easier for those who may have difficulty
            detecting who is speaking.</p>
    </div>
    <div class="w-container">
        <h2>Getting started</h2>
        <p class="paragraph-3">The goal of the project is:</p>
        <p class="quote"><em class="italic-text">Create a tool to help construct movie scripts from subtitles and images
                by automatically estimating who is delivering each subtitle line based on using biometrics to recognise
                the people present in each shot.</em></p>
        <p class="paragraph-3">My first task was to find a clip on YouTube that I would test my program on and download
            it. I settled with a movie clip from the Hunger Games where Katniss volunteers for the Games when her sister
            was chosen. It is a 3-minute long clip including 3 unique speakers.<br /><br />To download the clip and the
            auto-generated subtitles, I used youtube-dl. Using ffmpeg, I extracted every frame from the clip. The
            3-minute long video generated over 5000 images. See command below for how I did this:</p>
        <p class="code">ffmpeg -i &lt;FILE_PATH_TO_CLIP&gt; %04d.png</p>
    </div>
    <div class="w-container">
        <h2>Dependencies</h2>
        <h3>DeepFace</h3>
        <p class="paragraph-3">It was recommended to me that I use a Python library called DeepFace to carry out the
            face-recognition tasks. I had to install this on Google Colab as the library depends on tensorflow and keras
            which does not work with my Mac M1 chip. <br /><br />DeepFace is a lightweight face recognition and facial
            attribute analyis (age, gender, emotion and race) framework for python. I used this to retrieve embeddings
            of faces from images.</p>
        <h3>Pysub-Parser</h3>
        <p class="paragraph-3">Pysub-Parser is a Python library that parses subtitle scripts into a usable json format.
            I have used this library to parse the auto-generated subtitle script to be used within my program. It
            provides the start and end times of each segment as well as the text.</p>
        <h3>Face-Recognition</h3>
        <p class="paragraph-3">Face-Recognition claims to be the world&#x27;s simplest facial recognition api for
            Python. This library is used for comparing embeddings provided by DeepFace to identify cast members. This
            library was only used temporarily.</p>
        <h3>Numpy</h3>
        <p class="paragraph-3">Numpy is the fundamental package for scientific computing with Python. I use this library
            to format embeddings correctly for face-recognition.</p>
        <h3>SkLearn</h3>
        <p class="paragraph-3">SkLearn is a simple and efficient tool for predictive data-analysis. This library is used
            for clustering embeddings retrieved from DeepFace.</p>
        <h3>OpenCV (CV2)</h3>
        <p class="paragraph-3">OpenCV provides a real-time optimized Computer Vision library, tools, and hardware. I use
            this library for cropping the faces of cast members in the frames.</p>
    </div>
    <div class="w-container">
        <h2>The idea</h2>
        <p class="paragraph-3">I experimented with the DeepFace library a lot to try and understand all its
            functionalities and what would be best for me to use. I decided that I would use the library to get the
            biometric embeddings of each cast member. <br /></p>
        <p class="code">from deepface import DeepFace<br />from pysubparser import parser<br />import numpy as
            np<br />import face_recognition as fr<br />import json<br /><br />def getEmbedding(img):<br /> try:<br />
            embedding = DeepFace.represent(img_path = img, enforce_detection = False)<br /> return
            embedding[0][&quot;embedding&quot;]<br /> except:<br /> print(&quot;Image does not exist&quot;)<br /> return
            &quot;Face not found&quot;<br /><br /><br />def getCastEmbeddings():<br /> for index, cast in
            enumerate(castMetadata):<br /> eb = getEmbedding(cast[&quot;image&quot;])<br />
            castMetadata[index][&quot;embedding&quot;] = eb</p>
        <p class="paragraph-3">Above, you can see two methods. The first takes a parameter of an image file path. This
            will then be used by DeepFace&#x27;s represent() method to return the embedding of the face found in that
            image. The next method loops through castMetadata which is an array of objects holding data on each cast
            member. It calls the former method to get the embedding for each cast member and save it to
            castMetadata.<br /><br />The cast embeddings can now be compared with faces from the frames. However, I do
            not need to search every frame for a face. Only the frames with a speaking scene need to be searched.
            Therefore, I used the start and end times provided by pysub-parser to work out the first and last image of
            the segment. Every image between these will also be searched and embeddings provided.</p>
        <p class="code">imagesFolder = &lt;PATH_TO_FRAMES_FOLDER&gt;<br /><br />def
            embeddingsForSegment(subtitles):<br /> embeddings = []<br /><br /> for index, sub in
            enumerate(subtitles):<br />‍<br /> print(&quot;Starting subtitles index: &quot; + str(index))<br />‍<br />
            embeddings_segment = []<br /> start = sub.start<br /> end = sub.end<br /> fps = 24<br />‍<br /> # Starting
            image<br /> startSecond = (start.hour * 3600) + (start.minute * 60) + start.second + float(&quot;0.&quot; +
            str(start.microsecond))<br /> startCode = int(startSecond * fps)<br />‍<br /> # End image<br /> endSecond =
            (end.hour * 3600) + (end.minute * 60) + end.second + float(&quot;0.&quot; + str(end.microsecond))<br />
            endCode = int(endSecond * fps)<br />‍<br /> code = startCode<br /> while code &lt;= endCode:<br /> imgCode =
            formatImageCount(code)<br /> filePath = imagesFolder + &quot;/&quot; + imgCode + &quot;.png&quot;<br />
            current = getEmbedding(filePath)<br /> <br /> embeddings_segment.append({<br /> &quot;image&quot;:
            filePath,<br /> &quot;embedding&quot;: current<br /> })<br /> code += 1<br />‍<br />
            embeddings.append({<br /> &quot;id&quot;: sub.index,<br /> &quot;text&quot;: sub.text,<br />
            &quot;embeddings&quot;: embeddings_segment,<br /> &quot;start&quot;: start,<br /> &quot;end&quot;: end<br />
            })<br />‍<br /> return embeddings</p>
        <p class="paragraph-3">The code above shows how I retrieve the embeddings for the face found in each frame
            during a segment. The subtitle segment provided by pysub-parser is passed as a parameter (I will show you
            how I got this data later). It is used to get the code for the first and last image in the
            segment.<br /><br />Then, for every image between the first and final, we retrieve an embedding for a face
            found in that image. You will notice that I have used an unknown method called formatImageCount(). This is
            the code for that below:</p>
        <p class="code">def formatImageCount(count):<br /> newCount = count<br /> if count &lt; 10:<br /> newCount =
            &quot;000&quot; + str(count)<br /> elif count &lt; 100:<br /> newCount = &quot;00&quot; + str(count)<br />
            elif count &lt; 1000:<br /> newCount = &quot;0&quot; + str(count)<br /><br /> return str(newCount)</p>
        <p class="paragraph-3">The above method formats the image number to be provided as part of the file path to the
            image. Every image is 4 digits long.<br /><br />The next method I created was to find which cast member is
            seen the most in each segment. This involves using the embeddings to identify the cast members in each
            frame. See below:</p>
        <p class="code">def mostPresentInSegment(sub):<br /> current = {}<br />‍<br /> for image in
            sub[&quot;embeddings&quot;]:<br /> for cast in castMetadata:<br />‍<br /> try:<br /> results =
            fr.compare_faces([np.array(image[&quot;embedding&quot;])], np.array(cast[&quot;embedding&quot;]),
            tolerance=0.4)<br /> cast_name = cast[&#x27;name&#x27;]<br />‍<br /> if results[0] == True:<br /><br /> if
            cast_name in current.keys():<br /> current[cast_name][&#x27;count&#x27;] += 1<br /> else:<br />
            current[cast_name] = {}<br /> current[cast_name][&quot;name&quot;] = cast_name<br />
            current[cast_name][&#x27;count&#x27;] = 1<br />‍<br /> except:<br /> print(&quot;Error: Face not
            found&quot;)<br />‍<br /> highest_count = 0<br /> most_present_cast = {}<br />‍<br /> for cast_name in
            current.keys():<br /> if current[cast_name][&quot;count&quot;] &gt; highest_count:<br /> highest_count =
            current[cast_name][&quot;count&quot;]<br /> most_present_cast = current[cast_name]<br />‍<br /> return
            most_present_cast</p>
        <p class="paragraph-3">To find who is most present in a segment, I pass a subtitle segment to the method which
            is then used to loop through the embeddings which we retrieved earlier. Additionally, I want to loop through
            each of the cast members in castMetadata for each frame. Next, I compare the two embeddings using
            face-recognition&#x27;s compare_faces() method. This method has a strange format. The embeddings passed as
            parameters need to be numpy arrays, not standard Python lists. The first embeddings also has to be passed in
            square brackets. The final parameter, tolerance, allows me to adjust the strictness of the comparision. I
            found that 0.4 works the best.<br /><br />If the compare_faces() does not work, I have used exception
            handling (try and except) to ensure that the program does not crash. The method will return True or False
            depending on if it is a match or not. If it does return True, we increment the count for that cast member by
            1. The final part of the method loops through the cast members found in the segment and finds who has the
            highest count. That cast member is then returned.<br /><br />Now it is time to put it all together. The code
            below shows how each method is used.</p>
        <p class="code">scriptSrc = &lt;PATH_TO_SUBTITLE_FILE&gt;<br />script = parser.parse(scriptSrc)<br />‍<br />#
            Get embeddings<br />eb_segments = embeddingsForSegment(script)<br />getCastEmbeddings()<br />‍<br />data =
            []<br />for index, seg in enumerate(eb_segments):<br />‍<br /> print(&quot;Starting segment: &quot; +
            str(index))<br /> most_present = mostPresentInSegment(seg)<br /> data.append({<br /> &quot;cast&quot;:
            most_present,<br /> &quot;id&quot;: index,<br /> &quot;text&quot;: seg[&quot;text&quot;],<br />
            &quot;start&quot;: seg[&quot;start&quot;],<br /> &quot;end&quot;: seg[&quot;end&quot;]<br />
            })<br /><br />saveToJSON(data, &quot;segments&quot;)</p>
        <p class="paragraph-3">First, I parse the subtitles using pysub-parser which is then used to get the embeddings
            of the faces found in each frame. Next, I call getCastEmbeddings() method to save the embeddings for the
            cast members to castMetadata. I loop through the segments to call mostPresentInSegment() method and append
            the value to an array. Finally, you will notice that I have called an unseen method called saveToJson(). You
            can see this code below:</p>
        <p class="code">def saveToJSON(data, fileName):<br /> json_object = json.dumps(data, indent=4,
            default=&quot;str&quot;)<br />‍<br /> with open(fileName + &quot;.json&quot;, &quot;w&quot;) as
            outfile:<br /> outfile.write(json_object)</p>
        <p class="paragraph-3">This simple method takes two parameters, the data which you would like to save to json
            format and the file name of the json file. First, it turns the data into a json format using json.dumps().
            Then, it creates a new json file with the chosen file name and writes the data to it.</p>
    </div>
    <div class="w-container">
        <h2>Testing</h2>
        <p class="paragraph-3">I have retrieved the data required in a json format, I then needed to convert this into a
            readable format. It was suggested that I create an Excel sheet to represent this data. To do this, I used a
            python library called xlsxwriter which allows me to create and manipulate Excel spreadsheets. Below is the
            code to turn my data into a readable spreadsheet:</p>
        <p class="code">import json<br />import xlsxwriter<br />from PIL import Image<br /><br />dataFilePath =
            &lt;PATH_TO_JSON_DATA&gt;<br />frameFilePath = &lt;PATH_TO_FRAMES_FOLDER&gt;<br />frameExtension =
            &#x27;.png&#x27;<br /><br />def outputDataToExcel(data):<br />‍<br /> # Create Excel sheet<br />
            imgDimensionsSet = False<br /> workbook = xlsxwriter.Workbook(&#x27;results_VGG.xlsx&#x27;)<br /> worksheet
            = workbook.add_worksheet()<br />‍<br /> # Add titles<br /> bold = workbook.add_format({&#x27;bold&#x27;:
            True})<br /> worksheet.write(&#x27;A1&#x27;, &#x27;Subtitle&#x27;, bold)<br />
            worksheet.write(&#x27;B1&#x27;, &#x27;Image&#x27;, bold)<br /> worksheet.write(&#x27;C1&#x27;,
            &#x27;Speaker&#x27;, bold)<br /> worksheet.write(&#x27;D1&#x27;, &#x27;Count&#x27;, bold)<br />
            worksheet.write(&#x27;E1&#x27;, &#x27;Start&#x27;, bold)<br /> worksheet.write(&#x27;F1&#x27;,
            &#x27;End&#x27;, bold)<br />‍<br /> # Column widths<br /> worksheet.set_column(0, 0, 40)<br />
            worksheet.set_column(2, 6, 20)<br />‍<br /> for index, scene in enumerate(data):<br /> current = scene<br />
            worksheet.write(&#x27;A&#x27; + str(index + 2), current[&#x27;subtitle&#x27;])<br />
            worksheet.write(&#x27;C&#x27; + str(index + 2), current[&#x27;cast&#x27;][&#x27;name&#x27;])<br />
            worksheet.write(&#x27;D&#x27; + str(index + 2), current[&#x27;cast&#x27;][&#x27;count&#x27;])<br />
            worksheet.write(&#x27;E&#x27; + str(index + 2), current[&#x27;start&#x27;])<br />
            worksheet.write(&#x27;F&#x27; + str(index + 2), current[&#x27;end&#x27;])<br />‍<br /> if &#x27;image&#x27;
            in current.keys():<br />‍<br /> # Get image dimensions<br /> imagePath = frameFilePath +
            current[&#x27;image&#x27;] + frameExtension<br />‍<br /> if not imgDimensionsSet:<br /> img =
            Image.open(imagePath)<br /> worksheet.set_column(1, 1, img.width / 100 * 5)<br />
            worksheet.set_default_row(img.height / 100 * 20)<br /> worksheet.set_row(0, 15)<br /> imgDimensionsSet =
            True<br />‍<br /> worksheet.insert_image(&#x27;B&#x27; + str(index + 2), imagePath, {&#x27;x_scale&#x27;:
            0.2, &#x27;y_scale&#x27;: 0.2}) <br /><br />‍<br /> workbook.close()<br /> return {&#x27;status&#x27;:
            &#x27;Success&#x27;}<br /><br />def openData(filePath):<br /> f = open(filePath)<br /> data =
            json.load(f)<br />‍<br /> return data<br /><br />‍<br />if __name__ == &#x27;__main__&#x27;:<br /> jsonData
            = openData(dataFilePath)<br /> res = outputDataToExcel(jsonData)<br /> print(res[&#x27;status&#x27;])</p>
        <p class="paragraph-3">First, I retrieve the data from the json file using the openData() method. This takes one
            parameter, the file path to the data. Once the data has been retrieved, it is passed to outputDataToExcel()
            method which handles storing it in the spreadsheet. Firstly, this method creates the spreadsheet and formats
            it with headings for each column. Next, it loops through the data and outputs it to the spreadsheet using
            the .write() method. You will notice that it also has functionality to output an image to the spreadsheet. I
            have not yet implemented the system to provide an image to be displayed; this will be done further down the
            line. To output the image, I had to resize it by downscaling to ensure that the image does not take up too
            much space. As soon as all data has been outputted, the status response is outputted to console.</p>
    </div>
    <div class="w-container">
        <h2>Clusters and Embeddings</h2>
        <p class="paragraph-3">The code above has since been updated to improve the project&#x27;s efficiency and to
            provide more accurate results. I have created new and improved methods for retrieving, managing and
            processing the embeddings retrieved from DeepFace. Take a look at the code below:</p>
        <p class="code">numCast = 4<br />clipData = {<br /> &#x27;yt_channel&#x27;: &lt;YOUTUBE_CHANNEL&gt;<br />
            &#x27;title&#x27;: &lt;CLIP_TITLE&gt;<br /> &#x27;link&#x27;: &lt;LINK_TO_CLIP&gt;<br />}<br /><br /># Parse
            script<br />scriptSrc = &lt;PATH_TO_SUBTITLE_FILE&gt;<br />script = parser.parse(scriptSrc)<br />‍<br />#
            Get embeddings<br />eb_segments = embeddingsForSegment(script)<br />cast_cluster =
            getCastEmbeddings(eb_segments[&quot;total&quot;])<br />eb_segments =
            eb_segments[&quot;embeddings&quot;]<br />‍<br />data = []<br />for index, seg in
            enumerate(eb_segments):<br /> print(&quot;Starting segment: &quot; + str(index))<br /> most_present =
            mostPresentInSegment(seg, cast_cluster)<br /><br /> if &#x27;image&#x27; in most_present.keys():<br />
            save_matching_face_from_image(most_present[&quot;image&quot;], &quot;faceshots/&quot; + str(index) +
            &quot;.png&quot;, cast_cluster.cluster_centers_[int(most_present[&quot;id&quot;])])<br />‍<br />
            data.append({<br /> &quot;cast&quot;: most_present,<br /> &quot;id&quot;: index,<br /> &quot;text&quot;:
            seg[&quot;text&quot;],<br /> &quot;start&quot;: seg[&quot;start&quot;],<br /> &quot;end&quot;:
            seg[&quot;end&quot;]<br /> })<br />‍<br />saveToJSON(data, &quot;segments&quot;)</p>
        <p class="paragraph-3">Above is the main method of the system. You will notice that is has changed quite a bit
            from the previous. Firstly, we still need to parse the subtitle script. Next, we get the embeddings for the
            segments by passing the script as a parameter. This will return the embeddings for all the faces found in
            the frames. You can see the updated method below:</p>
        <p class="code">def embeddingsForSegment(subtitles):<br /> # Method to get embeddings for every frame in a
            subtitle segment<br />‍<br /> embeddings = []<br /> total_embeddings = [] ## Used for getting cast
            identities<br />‍<br /> for index, sub in enumerate(subtitles):<br /> print(&quot;Starting subtitles segment
            index: &quot; + str(index))<br /> embeddings_segment = []<br /> start = sub.start<br /> end =
            sub.end<br />‍<br /> # Starting image<br /> startSecond = (start.hour * 3600) + (start.minute * 60) +
            start.second + float(&quot;0.&quot; + str(start.microsecond))<br /> startCode = int(startSecond *
            24)<br />‍<br /> # End image<br /> endSecond = (end.hour * 3600) + (end.minute * 60) + end.second +
            float(&quot;0.&quot; + str(end.microsecond))<br /> endCode = int(endSecond * 24)<br />‍<br /> # Loop through
            each image from start of segment to end<br /> code = startCode<br /> while code &lt;= endCode:<br /> imgCode
            = formatImageCount(code)<br /> filePath = imagesFolder + &quot;/&quot; + imgCode + &quot;.png&quot;<br />
            current = getEmbedding(filePath)<br />‍<br /> if current != None:<br /> embeddings_segment.append({<br />
            &quot;image&quot;: filePath,<br /> &quot;img_code&quot;: imgCode,<br /> &quot;embedding&quot;: current<br />
            })<br /> total_embeddings.append(current)<br />‍<br /> code += 1<br />‍<br /> embeddings.append({<br />
            &quot;id&quot;: sub.index,<br /> &quot;text&quot;: sub.text,<br /> &quot;embeddings&quot;:
            embeddings_segment,<br /> &quot;start&quot;: start,<br /> &quot;end&quot;: end<br /> })<br />‍<br /> return
            {<br /> &quot;embeddings&quot;: embeddings,<br /> &quot;total&quot;: total_embeddings<br /> }</p>
        <p class="paragraph-3">The above method has not changed much. The main difference is that we are now returning
            two arrays in a dictionary rather than just 1. The &#x27;embeddings&#x27; key contains the embeddings
            seperated by segment, the &#x27;total&#x27; key contains all the embeddings together. <br />Looking back to
            the main method, I pass the total embeddings to the getCastEmbeddings method. Here is the updated method
            below:</p>
        <p class="code">def getEmbedding(img):<br /> # Method to retrieve embedding of face found in image<br />
            try:<br /> embedding = DeepFace.represent(img_path = img, enforce_detection = False)<br /> return
            embedding[0][&quot;embedding&quot;]<br /> except:<br /> print(&quot;Image does not exist&quot;)<br /> return
            None<br /><br /><br />def getCastEmbeddings(embeddings):<br /> # Method to get embeddings for each cast
            member<br />‍<br /> # Initialize KMeans with the desired number of clusters<br /> kmeans =
            KMeans(n_clusters=numCast, random_state=42)<br />‍<br /> # Convert the embeddings list to a numpy
            array<br /> embeddings_array = np.array(embeddings)<br />‍<br /> # Fit KMeans to the embeddings array<br />
            kmeans.fit(embeddings_array)<br />‍<br /> return kmeans</p>
        <p class="paragraph-3">While getEmbedding method has stayed the same, the getCastEmbeddings method has changed
            significantly. Previously, I was retrieving the embeddings for the cast members from images I downloaded
            from Google and saving it to a dictionary named castMetadata. I have now removed castMetadata. Instead, I
            use the clustering technique provided by sklearn&#x27;s kMeans property. This loops through all the
            embeddings passed as a parameter and identifies a certain amount of cast members by comparing the embeddings
            and finding the closest matching ones and &#x27;clustering&#x27; them together as a group.<br />‍<br />The
            reason behind this major change is to stop the need for cast member images to be provided and automate the
            system more. Now we need to look at how to compare the cluster of cast embeddings with the segment
            embeddings. Have a look at the updated mostPresentInSegment method:</p>
        <p class="code">def mostPresentInSegment(sub, cast_cluster):<br /> # Method to find who is seen most in each
            segment<br /> current = {}<br /> for image in sub[&quot;embeddings&quot;]:<br /> try:<br /> # Compare
            embedding from frame with each cast member&#x27;s embedding<br /> results, identity_index, bio_score =
            compareEmbeddings(image[&quot;embedding&quot;], cast_cluster.cluster_centers_, 0.4)<br />‍<br /> if results
            == True:<br />‍<br /> # If match, increment count for cast by 1<br /> if str(identity_index) in
            current.keys():<br /> current[str(identity_index)][&#x27;count&#x27;] += 1<br /> <br /> # If closer match,
            replace image<br /> if bio_score &gt; current[str(identity_index)][&quot;bio_score&quot;]:<br />
            current[str(identity_index)][&quot;bio_score&quot;] = bio_score <br />
            current[str(identity_index)][&quot;image&quot;] = image[&quot;image&quot;]
            current[str(identity_index)][&quot;img_code&quot;] = image[&quot;img_code&quot;]<br />‍<br /> else:<br />
            current[str(identity_index)] = {}<br /> current[str(identity_index)][&quot;id&quot;] = identity_index<br />
            current[str(identity_index)][&quot;count&quot;] = 1<br /> current[str(identity_index)][&quot;image&quot;] =
            image[&quot;image&quot;] current[str(identity_index)][&quot;img_code&quot;] = image[&quot;img_code&quot;]
            current[str(identity_index)][&quot;bio_score&quot;] = bio_score<br />‍<br /> except Exception as e:<br />
            print(&quot;ERROR: &quot; + str(e)) <br /> <br /> # Find who has highest count and return info<br />
            highest_count = 0<br /> most_present_cast = {}<br /> for identity in current.keys():<br /> if
            current[identity][&quot;count&quot;] &gt; highest_count:<br /> highest_count =
            current[identity][&quot;count&quot;]<br /> most_present_cast = current[identity]<br /> <br /> return
            most_present_cast</p>
        <p class="paragraph-3">The biggest change in this method is that I am no longer using the Face-Recognition
            library to compare embeddings. I have created my own embedding comparison method which you can see below. It
            returns 3 values, results determines whether the embedding could find a matching face in the cast cluster.
            identityIndex returns the id of the cast member found and bio_score is a float which represents how close
            the match was. The other change is that I return more values as part of the dictionary. These values include
            the cast member id, the number of times this cast member was found (count), the image and image code of the
            frame being compared and the biometric score. The image and image code is updated when a frame is found with
            a better biometric score.<br /><br />Take a look at the compareEmbeddings method below:</p>
        <p class="code">def compareEmbeddings(embedding, embedding_cluster, threshold):<br />‍<br /> # Calculate cosine
            similarity between the embedding and each embedding in the cluster similarities = np.dot(embedding,
            embedding_cluster.T) / (np.linalg.norm(embedding) * np.linalg.norm(embedding_cluster, axis=1))<br />‍<br />
            # Find the index of the embedding with the highest similarity<br /> max_index =
            np.argmax(similarities)<br />‍<br /> # Return True if the similarity is above a threshold, False
            otherwise<br /> if similarities[max_index] &gt; threshold:<br /> bio_score = similarities[max_index] -
            threshold<br /> return True, max_index, bio_score<br /> else:<br /> return False, None, -1</p>
        <p class="paragraph-3">This method uses the NumPy library to carry out calculations on the embeddings to
            determine whether the embedding from the frame can be closely linked to an embedding in the cast cluster.
            The threshold value is used as a cut off point for biometric scores. Any score value below the threshold
            will be discarded as not a match.<br /><br />The final update for this project is that I have created a
            method to cut out the face found in each segment and save it as a new image. This is so that I can determine
            which face is being detected in each segment as there may be many faces in the frame which is returned.</p>
        <p class="code">def save_matching_face_from_image(img_path, output_file_name, given_embedding):<br /> try:<br />
            # Load the image<br /> img = cv2.imread(img_path)<br />‍<br /> # Detect faces using the DeepFace
            library<br /> detected_faces = DeepFace.extract_faces(img_path=img)<br /> face_embeddings = []<br />
            cropped_faces = []<br />‍<br /> # Calculate embeddings for each detected face<br /> for face in
            detected_faces:<br /> x = face[&#x27;facial_area&#x27;][&#x27;x&#x27;]<br /> y =
            face[&#x27;facial_area&#x27;][&#x27;y&#x27;]<br /> w = face[&#x27;facial_area&#x27;][&#x27;w&#x27;]<br /> h
            = face[&#x27;facial_area&#x27;][&#x27;h&#x27;]<br />‍<br /> # Add padding to the cropping coordinates<br />
            x_padding = 50<br /> y_padding = 50<br /> x_start = max(0, x - x_padding)<br /> y_start = max(0, y -
            y_padding)<br /> x_end = min(img.shape[1], x + w + x_padding)<br /> y_end = min(img.shape[0], y + h +
            y_padding)<br />‍<br /> cropped_face = img[y_start:y_end, x_start:x_end]<br /> img_rgb =
            cv2.cvtColor(cropped_face, cv2.COLOR_BGR2RGB)<br /> embedding = DeepFace.represent(img_rgb,
            model_name=&#x27;VGG-Face&#x27;, enforce_detection=False)<br /> face_embeddings.append(embedding)<br />
            cropped_faces.append(cropped_face)<br />‍<br /> # Compare the calculated embeddings with the given face
            embedding<br /> min_distance = float(&#x27;inf&#x27;)<br /> best_match_index = None<br />‍<br /> for i,
            embedding in enumerate(face_embeddings):<br /> print(len(embedding))<br /> distance =
            np.linalg.norm(given_embedding - embedding[0][&#x27;embedding&#x27;])<br /> if distance &lt;
            min_distance:<br /> min_distance = distance<br /> best_match_index = i<br />‍<br /> # Save the matched face
            locally<br /> if best_match_index is not None:<br /> cv2.imwrite(output_file_name,
            cropped_faces[best_match_index])<br />‍<br /> except Exception as e:<br /> print(e)</p>
        <p class="paragraph-3">First, I load the image using OpenCV&#x27;s (CV2) imread() method. I then use DeepFace to
            detect all the faces in the image. Embeddings are calculated for each face detected as we need to compare it
            with the cluster which can only be done with another embedding. I add a padding of 50 pixels to the image as
            cast members can sometimes be unrecognisable if it is a really zoomed in image. For every face found in the
            image, it is compared with the cast cluster to find which is the closest match to one of the cast members.
            The result is then saved locally using OpenCV&#x27;s imwrite() method.<br /><br />The saveToJSON method has
            changed to allow clip data to be saved with the results:</p>
        <p class="code">def saveToJSON(data, fileName):<br /> # Method to save data to JSON file<br />‍<br /> output =
            {<br /> &#x27;clip_data&#x27;: clipData,<br /> &#x27;scenes&#x27;: data<br /> }<br /> json_object =
            json.dumps(output, indent=4, default=str)<br />‍<br /> with open(fileName + &quot;.json&quot;,
            &quot;w&quot;) as outfile:<br /> outfile.write(json_object)</p>
        <p class="paragraph-3">As the format of the json file has changed, I had to adapt the testing system to
            accomodate this:</p>
        <p class="code">import json<br />import xlsxwriterfrom PIL <br />import Image<br />import io<br />import
            os<br />‍<br />dataFilePath = &quot;&lt;PATH_TO_JSON_FILE&gt;&quot;<br />frameFilePath =
            &quot;&lt;PATH_TO_FRAMES_FOLDER&gt;&quot;<br />faceshotsFilePath =
            &quot;&lt;PATH_TO_FACESHOTS_FOLDER&gt;&quot;<br />frameExtension = &quot;.png&quot;<br />‍<br />def
            outputDataToExcel(data, fileName):<br />‍<br /> try:<br /> # Create Excel sheet<br /> imgDimensionsSet =
            False<br /> workbook = xlsxwriter.Workbook(fileName + &#x27;.xlsx&#x27;)<br /> worksheet =
            workbook.add_worksheet()<br /> workbook.formats[0].set_font_size(16)<br /> bold =
            workbook.add_format({&#x27;bold&#x27;: True})<br /> bold.set_font_size(18)<br />‍<br /> # Output clip
            data<br /> clip = data[&#x27;clip_data&#x27;]<br /> worksheet.write(&#x27;A1&#x27;, &#x27;Title:&#x27;,
            bold)<br /> worksheet.write(&#x27;B1&#x27;, clip[&#x27;title&#x27;])<br /> worksheet.write(&#x27;A2&#x27;,
            &#x27;Channel Name:&#x27;, bold)<br /> worksheet.write(&#x27;B2&#x27;, clip[&#x27;yt_channel&#x27;])<br />
            worksheet.write(&#x27;A3&#x27;, &#x27;Link to video:&#x27;, bold)<br /> worksheet.write(&#x27;B3&#x27;,
            clip[&#x27;link&#x27;])<br />‍<br /> # Add titles<br /> worksheet.write(&#x27;A4&#x27;, &#x27;Frame
            ID&#x27;, bold)<br /> worksheet.write(&#x27;B4&#x27;, &#x27;Subtitle&#x27;, bold)<br />
            worksheet.write(&#x27;C4&#x27;, &#x27;Frame&#x27;, bold)<br /> worksheet.write(&#x27;D4&#x27;, &#x27;Speaker
            Image&#x27;, bold)<br /> worksheet.write(&#x27;E4&#x27;, &#x27;Speaker ID&#x27;, bold)<br />
            worksheet.write(&#x27;F4&#x27;, &#x27;Count&#x27;, bold)<br />‍<br /> # No. times spotted in segment<br />
            worksheet.write(&#x27;G4&#x27;, &#x27;Biometric Score&#x27;, bold)<br /> worksheet.write(&#x27;H4&#x27;,
            &#x27;Start&#x27;, bold)<br /> worksheet.write(&#x27;I4&#x27;, &#x27;End&#x27;, bold)<br />‍<br /> # Column
            widths<br /> worksheet.set_column(0, 0, 20)<br /> worksheet.set_column(1, 3, 40)<br />
            worksheet.set_column(4, 8, 20)<br />‍<br /> for index, current in enumerate(data[&#x27;scenes&#x27;]):<br />
            worksheet.write(&#x27;A&#x27; + str(index + 5), current[&#x27;id&#x27;])<br /> worksheet.write(&#x27;B&#x27;
            + str(index + 5), current[&#x27;text&#x27;])<br /> worksheet.write(&#x27;H&#x27; + str(index + 5),
            current[&#x27;start&#x27;])<br /> worksheet.write(&#x27;I&#x27; + str(index + 5),
            current[&#x27;end&#x27;])<br />‍<br /> if len(current[&#x27;cast&#x27;].keys()) != 0:<br />
            worksheet.write(&#x27;E&#x27; + str(index + 5), current[&#x27;cast&#x27;][&#x27;id&#x27;])
            worksheet.write(&#x27;F&#x27; + str(index + 5), current[&#x27;cast&#x27;][&#x27;count&#x27;])<br />
            worksheet.write(&#x27;G&#x27; + str(index + 5),
            current[&#x27;cast&#x27;][&#x27;bio_score&#x27;])<br />‍<br /> # Get image dimensions<br /> imagePath =
            frameFilePath + current[&#x27;cast&#x27;][&#x27;img_code&#x27;] + frameExtension<br /> faceshotPath =
            faceshotsFilePath + str(index) + frameExtension<br /> if not imgDimensionsSet:<br /> img =
            Image.open(imagePath)<br /> worksheet.set_column(2, 3, img.width / 100 * 5)<br /> print(img.width / 100 *
            2)<br /> worksheet.set_default_row(img.height / 100 * 25)<br /> # worksheet.set_row(0, 15)<br />
            imgDimensionsSet = True<br />‍<br /> worksheet.insert_image(&#x27;C&#x27; + str(index + 5), imagePath,
            {&#x27;x_scale&#x27;: 0.25, &#x27;y_scale&#x27;: 0.25})<br />‍<br /> if os.path.exists(faceshotPath):<br />
            worksheet.insert_image(&#x27;D&#x27; + str(index + 5), faceshotPath, {&#x27;x_scale&#x27;: 0.5,
            &#x27;y_scale&#x27;: 0.5})<br /> else:<br /> worksheet.write(&#x27;D&#x27; + str(index + 5), &#x27;Faceshot
            Not Found&#x27;)<br />‍<br /> workbook.close()<br /> return {&#x27;status&#x27;: &#x27;Success&#x27;}<br />
            except Exception as e:<br /> print(e) <br /> return {&#x27;status&#x27;: &#x27;Failed&#x27;,
            &#x27;msg&#x27;: e} <br /><br />def openData(filePath):<br /> f = open(filePath)<br /> data =
            json.load(f)<br /> return data<br />‍<br />if __name__ == &#x27;__main__&#x27;:<br /> jsonData =
            openData(dataFilePath)<br /> res = outputDataToExcel(jsonData, &quot;Results_1&quot;)<br />
            print(res[&#x27;status&#x27;])<br /> if res[&#x27;status&#x27;] == &#x27;Failed&#x27;:<br />
            print(res[&#x27;msg&#x27;])</p>
        <p class="paragraph-3">The biggest change here is that I have added additional columns to support the extra data
            being outputted. This involved adding a column for the faceshots. The Excel sheet now shows the frame and
            faceshot with the highest biometric score from the cast member that was found the most in that segment. It
            also shows the number of times which they were seen in a frame and the biometric score itself. Another
            adaptation includes adding the clip data at the top of the spreadsheet.</p>
    </div>
    <div class="w-container">
        <h2>Conclusion</h2>
        <p class="paragraph-3">The system works well in identifying faces in the frames and determining who was seen the
            most during a segment. However, after testing, I have discovered some faults and inaccuracies. For scenes
            where there are many faces in the one frame, it is possible that the wrong face will be detected. This is
            not great for clustering cast embeddings together because background characters may be picked up the same
            amount of times as a main character and therefore, return the wrong identity. Moreover, if the cast
            member&#x27;s full face is not in the frame, it won&#x27;t be identified. For example, if a microphone is
            covering part of their face, they may not be picked up by DeepFace.</p>
    </div>
    <div class="w-container">
        <h2>What next?</h2>
        <p class="paragraph-3">As mentioned above, the system works, but it can be improved greatly by adding additional
            features. Adding a voice biometrics feature could help improve the accuracy of identifying the speaker.
            Currently, I can only identify faces seen during the clip; however, if a voice was to be paired with the
            face, the accuracy would be improved greatly. This could be done by retrieving the voice biometrics
            embeddings and clustering them together with the face biometrics embeddings. It has been suggested that by
            pairing these two biometrics together, they could be used to create voice and face deep fakes. This could be
            a great feature to allow someone to speak to the character after the movie and have them talk
            back.<br /><br />Furthermore, with the data gathered from this system, it can be experimented with AI
            chatbot such as ChatGPT. By providing the data, the chatbot could construct a screenplay which could then be
            used to manipulate the story and create more, related screenplays. Using a feature such as BLIP-2 to create
            descriptions for each character and providing this data to the chatbot could improve the
            screenplay.<br /><br />These are just some of the many different features which this AI-system could be used
            for. The uses for this system are endless.</p>
    </div>
    <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=63ff4af7985700a35d66e15f"
        type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0="
        crossorigin="anonymous"></script>
    <script src="./script.js"
        type="text/javascript"></script>
    <!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->
</body>

</html>